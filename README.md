# ğŸš– Uber Data Warehouse Implementation â€“ End-to-End Guide

A complete data warehousing project for Uber-style ride data, designed and deployed in **Google BigQuery**, with mock data generation and full analytical coverage of key business metrics.

---

## ğŸ“Œ 1. Project Overview

This project implements a **star schemaâ€“based data warehouse** for Uber operations using Google BigQuery. It supports real-time and historical analytics for the following business use cases:

- ğŸš— Ride duration and distance analysis  
- ğŸ’° Revenue by city and by driver  
- â° Peak time and surge pricing insights  
- ğŸ“ Popular pickup/drop-off route tracking  
- ğŸŒŸ Driver/rider rating metrics  
- âŒ Ride cancellation rate monitoring  
- ğŸŒ§ï¸ Weather impact on ride performance  
- ğŸ§‘â€ğŸ¤â€ğŸ§‘ Rider loyalty and churn metrics  

---

## ğŸ› ï¸ 2. Key Technologies

| Tool                | Purpose                          |
|---------------------|----------------------------------|
| **Google BigQuery** | Cloud data warehouse             |
| **Python 3.9+**      | Scripting and orchestration      |
| **Faker**            | Realistic mock data generation   |
| **Pandas**           | Data wrangling and DataFrames    |

---

## ğŸ—ƒï¸ 3. Schema Design

This project uses a **star schema** structure, with centralized fact tables and connected dimension tables.

### ğŸ“¦ Dimension Tables
- `drivers`
- `riders`
- `locations`
- `cities`
- `dates`
- `weather`

### ğŸ“Š Fact Tables
- `rides`
- `ride_status`

---

## âš™ï¸ 4. Setup Instructions

### ğŸ“Œ 1. Google Cloud Setup
1. Create a GCP Project
2. Enable the **BigQuery API**
3. Create a **Service Account** with `BigQuery Admin` role
4. Download the service account key (JSON)
5. Set the environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
````

> On Windows:

```cmd
set GOOGLE_APPLICATION_CREDENTIALS="C:\path\to\service-account.json"
```

---

### ğŸ 2. Python Environment

```bash
# Create virtual environment
python -m venv uber-dwh-env
source uber-dwh-env/bin/activate  # or .\uber-dwh-env\Scripts\activate on Windows

# Install dependencies
pip install pandas faker google-cloud-bigquery pyarrow
```

---

## ğŸ§ª 5. Data Generation & Loading

Run the mock data generation + BigQuery loading script:

```bash
python uber_mock_data_load_to_BQ.py
```

This will generate realistic drivers, riders, trips, and weather entries, and load them into your `uber_dw` BigQuery dataset.

---

## ğŸ“Š 6. Business Insights on Tables

Once data is loaded, run SQL queries in BigQuery like:

* Top 5 cities by total revenue
* Avg ride duration by time of day
* Most frequent pickup-dropoff location pairs
* Driver ratings and loyalty score correlations
* Weather-wise ride volume comparison

> You can also schedule these queries using **BigQuery Scheduled Queries** or external tools like **Data Studio** or **Looker**.

---

## â˜ï¸ 7. Deployment Architecture (Optional for Production)

For a production setup:

* ğŸ³ **Containerize** the pipeline using Docker
* ğŸ“… **Automate daily loads** using Cloud Scheduler + Cloud Functions
* ğŸ” **CI/CD pipelines** with Cloud Build + GitHub Actions
* ğŸ“ˆ **Monitoring** via Google Cloud Monitoring & Alerting

---

## ğŸ§  Author

Built by Pankaj as part of a hands-on data engineering portfolio.

---

